Implement the following plan:

# Fix "Fix with AI" — Targeted Line-Number-Based LaTeX Fixing

## Context

The "Fix with AI" button doesn't actually work reliably. The root cause is that the fix-latex route sends the **entire section content** (~2000-5000 lines) to Haiku with a vague error string like `"Compilation failed: ! Missing $ inserted; ! Undefined control sequence"`, and asks it to rewrite the **complete chapter**. This is:

- **Wasteful**: ~10,000 lines of AI I/O per fix
- **Error-prone**: Haiku adds preamble, changes content, or loses parts of the document
- **Imprecise**: The error string has no line numbers — just `"! Missing $ inserted"` with no location

Meanwhile, the `compilations` table already stores `log_text` (50KB of raw LaTeX log) which contains exact line numbers and file references, but fix-latex never uses it.

**Fixes 1-3 from the previous plan (preamble stripping, remove auto-recompile, isAutoCompiling state) are already implemented.** This plan addresses the core problem: making fix-latex actually work.

---

## Approach: Snippet-Based Targeted Fixing

Instead of full-document rewrite, extract only the lines around each error and fix those surgically.

### Layer 1: Enhance `parse-log.ts` — Extract Line Numbers + File Names

**File**: `apps/web/lib/latex/parse-log.ts`

Add `LatexErrorDetail` interface and `structuredErrors` to `ParsedLog`:

```typescript
export interface LatexErrorDetail {
  message: string;      // "Missing $ inserted"
  line?: number;        // 47 (from "l.47")
  file?: string;        // "chapters/results.tex" (from file stack)
  rawMessage: string;   // "! Missing $ inserted."
}

// Add to existing ParsedLog:
structuredErrors: LatexErrorDetail[];
```

Implementation:
- Track file stack via `(./filepath` and `)` parentheses in log
- After each `! error` line, scan next 4 lines for `l.NNN` pattern
- Record `{message, line, file}` in `structuredErrors`
- Keep existing `errors: string[]` unchanged for backward compatibility

### Layer 2: New Helper Module — `fix-latex-helpers.ts`

**New file**: `apps/web/lib/latex/fix-latex-helpers.ts`

Three helpers:

1. **`extractErrorContexts(content, errors)`** — For each error with a line number, extract ±5 lines around it. Merge overlapping windows.

2. **`buildTargetedUserMessage(contexts, errors)`** — Build focused AI prompt showing only the error snippets with line numbers:
   ```
   === Error: ! Missing $ inserted (line 47) ===
   42| Mean age was 45.3 years.
   ...
   47| Patients with BMI>30 kg/m2 had worse outcomes (p<0.001).
   ...
   52| The secondary analysis revealed:
   ```

3. **`parseFixResponse(response)` → `LineFix[]`** — Parse AI response lines matching `NN| fixed content`. Returns `null` if unparseable (triggers fallback).

4. **`applyLineFixes(original, fixes)`** — Replace specific lines by number in the original content.

### Layer 3: Rewrite `fix-latex/route.ts` — Dual-Path (Targeted + Fallback)

**File**: `apps/web/app/api/projects/[id]/sections/[phase]/fix-latex/route.ts`

1. Fetch latest failed compilation's `log_text` from `compilations` table
2. Parse with enhanced `parseLatexLog` → get structured errors with line numbers
3. Filter errors for this phase's chapter file (reverse-map via `PHASE_CHAPTER_MAP`)
4. If errors with line numbers found:
   - Extract context windows from section content body
   - Send focused prompt (only snippets) to Haiku with `max_tokens: 4096`
   - Parse response as `LineFix[]` and apply surgically
5. **Fallback** to current full-document approach if:
   - No compilation log in DB
   - Phase has no chapter file (phases 0, 1, 9, 10, 11)
   - No structured errors with line numbers
   - AI response doesn't parse as line fixes

### Layer 4: Export `PHASE_CHAPTER_MAP`

**File**: `apps/web/lib/latex/assemble.ts` — Add `export` to the existing `PHASE_CHAPTER_MAP` constant.

---

## Files to Modify/Create

| # | File | Change |
|---|------|--------|
| 1 | `apps/web/lib/latex/parse-log.ts` | Add `LatexErrorDetail` interface, `structuredErrors` field, file stack tracking, line number extraction |
| 2 | `apps/web/lib/latex/fix-latex-helpers.ts` | **NEW** — context extraction, prompt building, response parsing, line replacement |
| 3 | `apps/web/app/api/projects/[id]/sections/[phase]/fix-latex/route.ts` | Rewrite with targeted fix (DB log fetch → parse → snippets → AI → apply) + fallback |
| 4 | `apps/web/lib/latex/assemble.ts` | Export `PHASE_CHAPTER_MAP` (add `export` keyword) |
| 5 | `apps/web/lib/latex/parse-log.test.ts` | Add tests for structured errors, file tracking, line numbers |
| 6 | `apps/web/lib/latex/fix-latex-helpers.test.ts` | **NEW** — tests for context extraction, response parsing, line replacement |

**No frontend changes needed** — fix-latex route fetches the log from DB itself.

---

## AI Prompt Design (Targeted Mode)

**System prompt** — focused on returning only changed lines:
```
You are a LaTeX syntax repair tool. Return ONLY the fixed lines.

OUTPUT FORMAT:
NN| fixed line content

Only include lines you changed. If no changes needed: NO_CHANGES_NEEDED
```

**User message** — only the error snippets with ±5 lines context and line numbers.

This reduces AI I/O from ~10,000 lines to ~50 lines (**~200x token savings**).

---

## Key Design Decisions

1. **No frontend changes**: fix-latex fetches `log_text` from DB — simpler, no prop threading
2. **CONTEXT_RADIUS = 5**: Assembly transforms rarely shift lines by more than 2-3; 5-line window covers it
3. **Fallback is the current full-document approach**: any failure in the targeted path falls through gracefully
4. **`max_tokens: 4096`** for targeted (vs 16000 for full-doc): much cheaper, since only returning changed lines
5. **Phases 0, 1, 9, 10, 11 always use fallback**: they inject into main.tex, line numbers don't map to section content

---

## Verification

1. `pnpm --filter apollo-web exec tsc --noEmit` — 0 errors
2. `pnpm --filter apollo-web test` — all tests pass (including new parse-log and fix-latex-helpers tests)
3. E2E: Trigger compile error (e.g., bare `>` in text) → click "Fix with AI" → verify only the specific lines are changed, no preamble, no content rewrite
4. Test fallback: Phase 1 (front matter) error → click "Fix with AI" → falls back to full-document approach gracefully


If you need specific details from before exiting plan mode (like exact code snippets, error messages, or content you generated), read the full transcript at: /Users/devs/.REDACTED.jsonl

---

now audit the cahnges you just made

---

ok, now push to remote and watch CI

---

Ok, so now Check this. the Inngest API is taking so long! is it the model output taking the time, or is the API itself slow? is it me , cluade or inngest is the question?

---

[Image source: REDACTED 2026-02-22 at 4.17.28 PM.png]

---

yes check the supabase region

---

so what can i do without switching db or vps provider?

---

audit yourself again, think ultra hard, did you break anything?

---

Your upsert change in thesis-workflow.ts is excellent. Compressing a SELECT and an INSERT into a single atomic operation is the textbook way to handle high-latency database connections. Keep that exactly as is.

However, your proposed changes to ai-generate.ts misunderstand the core issue. The bottleneck is the 600ms physical network latency to Mumbai, not Inngest's ~8ms execution overhead. Merging steps to save a few milliseconds of Inngest overhead while degrading the app's UX and reliability is the wrong tradeoff.

Please revert your changes to ai-generate.ts and implement the following architecture instead:

1. Revert the Step Merging (Keep Steps Separate) Your math on the step merge is flawed. Because the Supabase calls (sections.update and recordTokenUsage) run sequentially inside your merged step, they still take ~1.2s total. You only saved ~16ms of Inngest overhead.

By wrapping recordTokenUsage in a try/catch, you are silently dropping usage/billing data on network timeouts.
If the save fails but the token log succeeds, Inngest retries the whole step, which runs the token log again and double-charges the user.
The Fix: Revert back to the separate steps: bibtex-integrity, save-content, and record-tokens. Let the background worker take an extra 16ms so we keep Inngest's exact failure isolation, avoid billing bugs, and don't re-run expensive AI/BibTeX tasks just because a DB insert timed out.
2. Revert the Flush Interval (It breaks the UX) Changing the flush interval to 2000 characters is roughly 500 words. This means the UI will sit frozen for 10-20 seconds and then suddenly dump half a page of text. This ruins the "smooth typing" effect of LLM streaming.

The Fix: Put the interval back down to a UX-friendly number (e.g., 100 to 200 characters).
3. The Actual Solution: Non-blocking DB Writes The real way to solve the streaming latency is to change how the code waits for the database. Right now, the stream halts for 600ms on every chunk to wait for Mumbai to confirm the write.

The Fix: Do not await the Supabase PATCH/update request inside the messageStream.on("text") loop. You must decouple the stream from the database. The text should immediately stream to the client via Realtime, while the database writes are fired as asynchronous, un-awaited promises (fire-and-forget). This allows the frontend to stream flawlessly at LLM speed, while Mumbai catches up in the background.
Please rewrite ai-generate.ts to keep the Inngest steps separated, restore the small flush interval, and implement the non-blocking fire-and-forget pattern for the streaming database writes.

---

push and watch CI